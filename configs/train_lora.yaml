output_dir: outputs/checkpoints/default_lora
num_train_epochs: 3
learning_rate: 1e-4
lr_scheduler_type: cosine
warmup_steps: 100
weight_decay: 0.01
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 16
max_grad_norm: 1.0
eval_strategy: steps
eval_steps: 200
logging_steps: 20
save_strategy: steps
save_steps: 200
save_total_limit: 3
bf16: true
fp16: false
seed: 42
report_to:
  - none
lora:
  r: 64
  alpha: 16
  dropout: 0.05
  bias: none
  task_type: CAUSAL_LM
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
  use_gradient_checkpointing: true
  merge_weights: false
  save_merged_weights: false
fsdp_config: null
deepspeed_config: null
