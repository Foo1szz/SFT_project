output_dir: outputs/checkpoints/default_full
num_train_epochs: 3
learning_rate: 2e-5
lr_scheduler_type: cosine
warmup_ratio: 0.03
weight_decay: 0.01
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 32
max_grad_norm: 0.3
eval_strategy: steps
eval_steps: 500
logging_steps: 20
save_strategy: steps
save_steps: 500
save_total_limit: 2
bf16: true
fp16: false
seed: 42
report_to:
  - none
fsdp_config:
  fsdp: "full_shard auto_wrap"
  fsdp_transformer_layer_cls_to_wrap: LlamaDecoderLayer
  auto_wrap_policy: transformer_layer
  backward_prefetch: BACKWARD_PRE
  activation_checkpointing: true
  activation_checkpointing_reentrant: false
  state_dict_type: FULL_STATE_DICT
deepspeed_config: null
lora: null
